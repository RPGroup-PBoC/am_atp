{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File with final parameter estimation code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "# Justin's packages - needs bebi103 environment\n",
    "# import iqplot\n",
    "# import bebi103\n",
    "\n",
    "# bokeh.io.output_notebook()\n",
    "\n",
    "# Import seaborn for aesthetic plots \n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# from bokeh.plotting import figure, show, curdoc\n",
    "# from bokeh.io import output_notebook\n",
    "# from bokeh.layouts import gridplot\n",
    "# from bokeh.plotting import figure, show\n",
    "# from bokeh.io import output_notebook\n",
    "# from bokeh.models import ColorBar\n",
    "# from bokeh.transform import linear_cmap\n",
    "# from bokeh.palettes import Viridis256\n",
    "# from bokeh.themes import Theme\n",
    "# from bokeh.layouts import column, row\n",
    "# output_notebook()\n",
    "\n",
    "# from bokeh.models import ColorBar, LinearColorMapper\n",
    "# from bokeh.palettes import Viridis256, Cividis256, Plasma256, Magma256, Blues8, BuRd\n",
    "# from bokeh.models import ColumnDataSource\n",
    "\n",
    "# from bokeh.models import BasicTicker, PrintfTickFormatter\n",
    "\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "from scipy.optimize import minimize\n",
    "# import statsmodels.tools.numdiff as smnd\n",
    "\n",
    "# Import time\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Numpy imports:    \n",
    "import numpy as np\n",
    "\n",
    "# Pandas for csv \n",
    "import pandas as pd\n",
    "\n",
    "# for extracting filenames \n",
    "import glob\n",
    "\n",
    "#Matplotlib imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# skimage submodules we need\n",
    "import skimage.io\n",
    "\n",
    "#Scipy imports\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import os\n",
    "\n",
    "# import atp_hydro\n",
    "# atp_hydro.pboc_style_mpl()\n",
    "# show images in viridis by default (pboc is fire?)\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "\n",
    "# Import seaborn for aesthetic plots \n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#for saving\n",
    "import h5py\n",
    "\n",
    "# Plotting params\n",
    "size = 500;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(datafolder, skip_int = 1, max_image_number = 300):\n",
    "    # bound Images\n",
    "    included_bound = '*405*.tif'\n",
    "    bound_files = np.sort(glob.glob(datafolder+'/'+included_bound))[:max_image_number*skip_int:skip_int]\n",
    "    bound_images = [skimage.io.imread(image_location) for image_location in bound_files]; \n",
    "    # unbound Images\n",
    "    included_unbound = '*480*.tif'\n",
    "    unbound_files = np.sort(glob.glob(datafolder+'/'+included_unbound))[:max_image_number*skip_int:skip_int]\n",
    "    unbound_images = [skimage.io.imread(image_location) for image_location in unbound_files]; \n",
    "\n",
    "    return bound_images, unbound_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_dark_avg(file_path_dark):\n",
    "    \"\"\"\n",
    "    Imports a dark image.\n",
    " \n",
    "    Parameters:\n",
    "    file_path_dark (string): Data path for a dark image.\n",
    "    \n",
    "    Returns:\n",
    "    dark_avg (numpy.ndarray): Dark image as a 2D array.\n",
    "    \"\"\"\n",
    "    dark_files = np.sort(glob.glob(file_path_dark))\n",
    "    dark_ims = np.array([skimage.io.imread(image_location) for image_location in dark_files]); \n",
    "    dark_avg = np.average(dark_ims, axis=0)\n",
    "    return dark_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bivariate_quadratic(image):\n",
    "    \"\"\"\n",
    "    Fits a bivariate quadratic polynomial to the intensity values of a grayscale image.\n",
    "    \n",
    "    Parameters:\n",
    "    image (numpy.ndarray): Grayscale image of shape (M, N).\n",
    "    \n",
    "    Returns:\n",
    "    coefficients (numpy.ndarray): Coefficients of the fitted polynomial [a00, a10, a01, a20, a11, a02].\n",
    "    \"\"\"\n",
    "    # Get image dimensions\n",
    "    M, N = image.shape\n",
    "\n",
    "    # Create a meshgrid of coordinates\n",
    "    i = np.arange(M)\n",
    "    j = np.arange(N)\n",
    "    I, J = np.meshgrid(i, j, indexing='ij')\n",
    "\n",
    "    # Flatten the matrices for the least squares fitting\n",
    "    I_flat = I.flatten()\n",
    "    J_flat = J.flatten()\n",
    "    Z_flat = image.flatten()\n",
    "\n",
    "    # Create the design matrix for the polynomial terms\n",
    "    A = np.vstack([I_flat**2, J_flat**2, I_flat*J_flat, I_flat, J_flat, np.ones_like(I_flat)]).T\n",
    "\n",
    "    # Solve for the coefficients using least squares\n",
    "    coefficients, _, _, _ = np.linalg.lstsq(A, Z_flat, rcond=None)\n",
    "\n",
    "    return coefficients\n",
    "\n",
    "def evaluateBivPoly(image):\n",
    "    \"\"\" Evaluates best-fit bivariate polynomial for image. \"\"\"\n",
    "    coefficients=fit_bivariate_quadratic(image);\n",
    "    I, J = np.meshgrid(np.arange(image.shape[0]), np.arange(image.shape[1]), indexing='ij')\n",
    "    Z_fitted = (coefficients[0] * I**2 + coefficients[1] * J**2 +\n",
    "            coefficients[2] * I * J + coefficients[3] * I +\n",
    "            coefficients[4] * J + coefficients[5])\n",
    "    return Z_fitted\n",
    "\n",
    "def norm_unev(arrs_sub):\n",
    "    \"\"\"\n",
    "    Corrects uneven illumination by fitting an image to a bivariate polynomial, normalizing the polynomial, and dividing an array of images by this normalized matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    arrs_sub (list): Dark image subtracted list of image arrays.\n",
    "    allmask_coords (slice): slice containing all the illumination coordinates with an extra dimension to account for all timepoints\n",
    "    \n",
    "    Returns:\n",
    "    arr_ev (list): list of image arrays after the evening process.\n",
    "    \"\"\"\n",
    "    #norm_mats = []\n",
    "    arrs_ev = []\n",
    "    \n",
    "    for arr in arrs_sub: # Iterate over channels\n",
    "        #print(\"time before polynomial fitting\", time.time()); \n",
    "        #compute the bivariate filter\n",
    "        biv_filt = np.zeros_like(arr[0]) \n",
    "        biv_filt = evaluateBivPoly(arr[0, :, :])\n",
    "        #norm_mats.append(norm_mat)\n",
    "        #print(\"time after polynomial fitting\", time.time());\n",
    "        #scale the normalization matrix such that the average value after multiplication is the same as the bs image\n",
    "\n",
    "        scalar = np.mean(arr[0])/np.mean(arr[0]/biv_filt)\n",
    "\n",
    "        # Normalisation\n",
    "        norm_mat = scalar/biv_filt\n",
    "        print(\"shape of norm mat\", norm_mat.shape)\n",
    "        print(\"shape of arr\", arr.shape)\n",
    "        print(\"time after normalising matrix\", time.time());\n",
    "\n",
    "        arrs_ev_temp = np.zeros_like(arr)\n",
    "        arrs_ev_temp=arr*norm_mat; \n",
    "        print(\"time after multiplying arr with norm_mat\", time.time());\n",
    "        \n",
    "        arrs_ev.append(arrs_ev_temp)\n",
    "        print(\"time after appending\", time.time());\n",
    "        \n",
    "    return arrs_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_calparams(file_path_cal):\n",
    "    \"\"\"\n",
    "    Imports ATP calibration parameters as a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path_cal (string): Data path to a text file with fitting parameters for ATP calibration\n",
    "    \n",
    "    Returns:\n",
    "    cal_params (pd.DataFrame): DataFrame with ATP calibration fitting parameters.\n",
    "    \"\"\"\n",
    "        \n",
    "    #read the DataFrame\n",
    "    #cal_params = pd.read_csv(cal_file_path+cal_file_folder+cal_file_name)\\\n",
    "    cal_params = pd.read_csv(file_path_cal)\n",
    "\n",
    "    #add hill=1 if no hill specified\n",
    "    if len(cal_params)==3:\n",
    "        cal_params.loc[-1] = [3, 'nhill', 1, 0]\n",
    "    \n",
    "    #reindex\n",
    "    cal_params = cal_params.set_index('Param')\n",
    "    \n",
    "    #drop the unnamed colum (from index of text file)\n",
    "    cal_params = cal_params.drop(columns='Unnamed: 0')\n",
    "    \n",
    "    return cal_params\n",
    "\n",
    "def replace_out_of_range_values(arr, min_value, max_value):\n",
    "    \"\"\"\n",
    "    Crop the intensities of an array to be withing a defined range.\n",
    "    \n",
    "    Parameters:\n",
    "    arr (numpy.ndarray): images array.\n",
    "    min_value (float): minimum value below which all other values are set to the minimum\n",
    "    max_value (float): maximum value above which all other values are set to the maximum\n",
    "    \n",
    "    Returns:\n",
    "    arr_copy (numpy.ndarray): image array cropped to the set intensity range.\n",
    "    \"\"\"\n",
    "    arr_copy = np.copy(arr)\n",
    "    mask_min = arr < min_value\n",
    "    mask_max = arr > max_value\n",
    "    arr_copy[mask_min] = min_value\n",
    "    arr_copy[mask_max] = max_value\n",
    "    return arr_copy\n",
    "\n",
    "def ATP_conc_to_ratio(array, Km, Rmax, Rmin, nhill):\n",
    "    \"\"\"Given a set of concentrations, returns ratio values based on provided Hill equation coefficients\"\"\"\n",
    "    return (Rmax-Rmin)*((array/Km)**nhill/(1 + (array/Km))**nhill) + Rmin\n",
    "\n",
    "def ATP_ratio_to_conc(array, Km, Rmax, Rmin, nhill):\n",
    "    \"\"\"Given a set of ratios, returns concentration values based on provided Hill equation coefficients\"\"\"\n",
    "    return Km * ((Rmin - array) / (array - Rmax)) ** (1/nhill)\n",
    "\n",
    "def infer_concs(ratios, exp_params, cal_params, shape):\n",
    "    \"\"\"\n",
    "    Convert ratios images to concentrations. (outputs an image array)\n",
    "    \n",
    "    Parameters:\n",
    "    ratios (numpy.ndarray): ratio array of shape (# timepoints, #of ratio values to convert).\n",
    "    exp_params (dictionary): dictonary of experimental parameters\n",
    "    cal_params (dictionary): maximum value above which all other values are set to the maximum\n",
    "    allmask_coords (slice): coordinates for multiple timepoints \n",
    "    shape (tuple): shape of image array\n",
    "    \n",
    "    Returns:\n",
    "    ratios_conc_ims (numpy.ndarray): image array in concentration units.\n",
    "    \"\"\"\n",
    "    #Find the ratio value for the inital ATP conc value based on calibration curve equation\n",
    "\n",
    "    print(\"Find the ratio value for the inital ATP conc value based on calibration curve equation\")\n",
    "    RinitATP = ATP_conc_to_ratio(exp_params['ATP_conc'], \n",
    "                                 cal_params.loc['Km']['Value'], \n",
    "                                 cal_params.loc['Rmax']['Value'],\n",
    "                                 cal_params.loc['Rmin']['Value'], \n",
    "                                 cal_params.loc['nhill']['Value'])\n",
    "    #Crop the ratios to within range\n",
    "    print(\"Crop the ratios to within range\")\n",
    "    ratios_inrange = replace_out_of_range_values(ratios, \n",
    "                                                 cal_params.loc['Rmin']['Value'], \n",
    "                                                 RinitATP)\n",
    "    #convert ratios to concentrations\n",
    "    print(\"convert ratios to concentrations\")\n",
    "    ratios_conc = ATP_ratio_to_conc(ratios_inrange, cal_params.loc['Km']['Value'], \n",
    "                                     cal_params.loc['Rmax']['Value'],\n",
    "                                     cal_params.loc['Rmin']['Value'], \n",
    "                                     cal_params.loc['nhill']['Value'])\n",
    "    \n",
    "    return ratios_conc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(datafolder, skip_int = 1, max_image_number = 300, frame_interval = 20):\n",
    "    now = time.time()\n",
    "    print(\"time\", now)\n",
    "    #--------- Read Images -----------#\n",
    "    bound_images, unbound_images = read_images(datafolder, skip_int, max_image_number); \n",
    "    bound_images = np.array(bound_images, dtype = np.float32); \n",
    "    unbound_images = np.array(unbound_images, dtype = np.float32); \n",
    "    \n",
    "    #--------- Read Dark Files -----------#\n",
    "    dark_avg = grab_dark_avg('../../data/dark_ims/2021-01-13_nocamera_dark_1/*Pos*/*tif*');\n",
    "    dark_avg = dark_avg.astype(np.float32); \n",
    "    \n",
    "    #--------- Subtract Dark Image Background from Experiment Images -----------#\n",
    "    bound_bs = bound_images - dark_avg; \n",
    "    unbound_bs = unbound_images - dark_avg; \n",
    "    print(\"normalisation starting in \", now - time.time(), \" s\"); \n",
    "\n",
    "    #--------- Camera has Uneven Illumincation. Correct this Unevent Illumination in Background Subtracted Images -----------#\n",
    "    [even_bound_bs, even_unbound_bs] = norm_unev([bound_bs, unbound_bs]); \n",
    "    even_bound_bs = even_bound_bs.astype(np.float32)\n",
    "    even_unbound_bs = even_unbound_bs.astype(np.float32)\n",
    "    print(\"normalisation finished in \", now - time.time(), \" s\"); \n",
    "    print(\"float type bound bs\", type(even_bound_bs[0][0][0]))\n",
    "    #RETURN NORM MAT\n",
    "    #--------- Calculate Ratios -----------#\n",
    "    ratios_array = even_bound_bs/even_unbound_bs; \n",
    "    ratios_array = ratios_array.astype(np.float32)\n",
    "    print(\"rations calculated\")\n",
    "    print(\"float type ratios array\", type(ratios_array[0][0][0]))\n",
    "\n",
    "    #--------- Convert Ratio to ATP -----------#\n",
    "    cal_file_path = '../../analyzed_data/atp_cal/' #RERUN\n",
    "    cal_file_folder = '2023-12-16_A81D_Cal/'\n",
    "    cal_file_name = 'df_fit_example'\n",
    "    cal_dir = cal_file_path+cal_file_folder+cal_file_name; \n",
    "\n",
    "    cal_params = grab_calparams(cal_dir); \n",
    "    print(\"cal params extracted\")\n",
    "\n",
    "\n",
    "    # Convert ratios to atp\n",
    "    atp0 = 1000; #in uM. TODO: Hardcoded for this example but needs to be generalised.\n",
    "    exp_params = {\n",
    "        \"ATP_conc\": atp0,\n",
    "    }; \n",
    "\n",
    "    ratios_conc_ims = infer_concs(ratios_array, exp_params, cal_params, ratios_array[0].shape); \n",
    "\n",
    "    # -------------- Save processed data -------------- #\n",
    "\n",
    "    # Initialise processed_data variable\n",
    "    processed_data = {}; \n",
    "\n",
    "    experiment_type = datafolder.split(\"/\")[0];  \n",
    "    # If looking at ADP variation\n",
    "    if experiment_type == \"ATP\": \n",
    "        try:\n",
    "            ATP0 = int(datafolder.split(\"/\")[-2].split(\"uMATP\")[0])\n",
    "        except:\n",
    "            ATP0 = np.nan; \n",
    "        ADP0 = 0; \n",
    "        P0 = 0\n",
    "    elif experiment_type == \"ADP\": \n",
    "        ATP0 = float(datafolder.split(\"uMATP\")[0].split(\"_\")[-1]); \n",
    "        ADP0 = float(datafolder.split(\"uMADP\")[0].split(\"/\")[-1]); \n",
    "        P0 = 0; \n",
    "    elif experiment_type == \"Phosphate\": \n",
    "        ATP0 = int(datafolder.split(\"/\")[1].split(\"_\")[-1].split(\"uMATP\")[0]); \n",
    "        ADP0 = 0; \n",
    "        P0 = int(datafolder.split(\"/\")[-2].split(\"mMPi\")[0]); \n",
    "    else: \n",
    "        raise Exception(\"Unidentified experiment type.\")\n",
    "    \n",
    "    # Dictionary to store arrays from each file\n",
    "    processed_data[\"ATP0 (uM)\"] = ATP0; \n",
    "    processed_data[\"ADP0 (uM)\"] = ADP0; \n",
    "    processed_data[\"P0 (uM)\"] = P0; \n",
    "    processed_data[\"Motor Concentration (uM)\"] = 1; \n",
    "    processed_data[\"ATP Curve (uM)\"] = ratios_conc_ims; \n",
    "    processed_data[\"Frame Interval (s)\"] = int(datafolder.split(\"/\")[2].split(\"_\")[3].split(\"sFrameInterval\")[0]); \n",
    "    \n",
    "    print(processed_data); \n",
    "\n",
    "    #split the hard drive name\n",
    "    local_dirprefix = '../../analyzed_data/atp_hydro/'\n",
    "    local_dir = local_dirprefix + datafolder.split('external/')[1]\n",
    "    os.system(\"mkdir -p \"+local_dir)\n",
    "\n",
    "    #define time of save\n",
    "    now = datetime.now()\n",
    "    save_date = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # permits saving into the new directory we just created\n",
    "    # #localNewDirName=desiredSavePathName.split('/')[-2]\n",
    "    # with h5py.File(local_dir+save_date+'even_bound_bs.h5', 'w') as h5file: \n",
    "    #     h5file.create_dataset(local_dir+save_date+'even_bound_bs', data=even_bound_bs)\n",
    "\n",
    "    #save csv START HERE ANA\n",
    "    testdf = pd.DataFrame(np.arange(10))\n",
    "    testdf.to_csv(local_dirprefix+'testdf.csv')\n",
    "\n",
    "      # # permits saving into the new directory we just created\n",
    "    # #localNewDirName=desiredSavePathName.split('/')[-2]\n",
    "    # with h5py.File(local_dir+save_date+'even_bound_bs.h5', 'w') as h5file: \n",
    "    #     h5file.create_dataset(local_dir+save_date+'even_bound_bs', data=even_bound_bs)\n",
    "\n",
    "    # with h5py.File(local_dir+save_date+'even_unbound_bs.h5', 'w') as h5file: \n",
    "    #     h5file.create_dataset(local_dir+save_date+'even_unbound_bs', data=even_unbound_bs)\n",
    "\n",
    "    # with h5py.File(local_dir+save_date+'ratios_conc_ims.h5', 'w') as h5file: \n",
    "    #     h5file.create_dataset(local_dir+save_date+'ratios_conc_ims', data=ratios_conc_ims)\n",
    "\n",
    "    # #save the frame interval (time between consecutive images)\n",
    "    # with h5py.File(local_dir+save_date+'frame_interval.h5', 'w') as h5file: \n",
    "    #     h5file.create_dataset(local_dir+save_date+'frame_interval', data=frame_interval)\n",
    "    # #save the skip interval (the number of frames between analyzed points)\n",
    "    # with h5py.File(local_dir+save_date+'skip_int.h5', 'w') as h5file: \n",
    "    #     h5file.create_dataset(local_dir+save_date+'skip_int', data=skip_int)\n",
    "\n",
    "    return bound_images, unbound_images, bound_bs, unbound_bs, even_bound_bs, even_unbound_bs, ratios_conc_ims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  0\n",
       "1  1\n",
       "2  2\n",
       "3  3\n",
       "4  4\n",
       "5  5\n",
       "6  6\n",
       "7  7\n",
       "8  8\n",
       "9  9"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_location = \"../../data/atp_hydro/1000uM_ATP_0_ADP_0_P/\"; # Specify data location\n",
    "# bound_images, unbound_images, bound_bs, unbound_bs, even_bound_bs, even_unbound_bs, ratios_conc_ims = process_folder(data_location); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read csv file containing meta information of experimental data location. \n",
    "csv_file_location = \"../../data/atp_hydro/file_directories_NajmaHydrolysis.csv\"; \n",
    "filelocs = pd.read_csv(csv_file_location); \n",
    "\n",
    "haschild = filelocs[~filelocs['child'].isna()]\n",
    "haschildList = list(haschild['Species']+'/'+haschild['parent']+'/'+haschild['child']+'/'+haschild['variant']+'/')\n",
    "childless = filelocs[filelocs['child'].isna()]\n",
    "childlessLess = list(childless['Species']+'/'+childless['parent']+'/'+childless['variant']+'/')\n",
    "\n",
    "\n",
    "filelocList = haschildList; # Don't look at folder names without nikon information for the moment.\n",
    "\n",
    "# for file in filelocList: \n",
    "#     process_folder(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will live in a repo on delbruck whcich will theoretically be located \n",
    "# in the am_atp folder in the \"aduarte\" or \"minakshi\" folder. Our accounts are located\n",
    "# in the home folder on delbruck organized like \"home/aduarte\". At the same level is the \n",
    "# mounted drive under the folder \"mnt\"\n",
    "pathToHome = '../../../../' #aduarte/am_atp/analysis/atp_hydro/\n",
    "extDrivePathFromHome = '../../mnt/external/Najma_Hydrolysis/'\n",
    "# testpath = pathToHome+extDrivePathFromHome+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read files directly from hard drive\n",
    "\n",
    "frame_int = 20 #s\n",
    "Motconc = 1 #uM, NCD Motors \n",
    "skip_int = 5 #data frames to skip \n",
    "\n",
    "# Declare where data is stored \n",
    "data_location = '/Volumes/Ana_AM/Najma_Hydrolysis/ADP/1_ADP_variation_at_500uMATP/Nikon_10X_bin1_20sFrameInterval__100ms480_150ms405_500uATP_1uMmicro_1400nM_A81D_1/0uMADP/'\n",
    "\n",
    "bound_images, unbound_images, bound_bs, unbound_bs, even_bound_bs, even_unbound_bs, ratios_conc_ims = process_folder(data_location, skip_int=skip_int); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.mean(ratios_conc_ims, axis=(1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_path = '../../analyzed_data/atp_hydro/'\n",
    "\n",
    "# #define time of save\n",
    "# now = datetime.now()\n",
    "# save_date = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# #define frame interval\n",
    "# frame_interval = data_location.split('sFrameInterval')[0][-2:]\n",
    "\n",
    "# # permits saving into the new directory we just created\n",
    "# #localNewDirName=desiredSavePathName.split('/')[-2]\n",
    "# with h5py.File(temp_path+save_date+'even_bound_bs.h5', 'w') as h5file: \n",
    "#     h5file.create_dataset(temp_path+save_date+'even_bound_bs', data=even_bound_bs)\n",
    "\n",
    "# with h5py.File(temp_path+save_date+'even_unbound_bs.h5', 'w') as h5file: \n",
    "#     h5file.create_dataset(temp_path+save_date+'even_unbound_bs', data=even_unbound_bs)\n",
    "# #save the ratios array uM units\n",
    "# with h5py.File(temp_path+save_date+'ratios_conc_ims.h5', 'w') as h5file: \n",
    "#     h5file.create_dataset(temp_path+save_date+'ratios_conc_ims', data=ratios_conc_ims)\n",
    "\n",
    "# #save the frame interval (time between consecutive images)\n",
    "# with h5py.File(temp_path+save_date+'frame_interval.h5', 'w') as h5file: \n",
    "#     h5file.create_dataset(temp_path+save_date+'frame_interval', data=frame_interval)\n",
    "# #save the skip interval (the number of frames between analyzed points)\n",
    "# with h5py.File(temp_path+save_date+'skip_int.h5', 'w') as h5file: \n",
    "#     h5file.create_dataset(temp_path+save_date+'skip_int', data=skip_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Processed Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the hard drive name\n",
    "local_dirprefix = '../../analyzed_data/atp_hydro/'\n",
    "local_dir = local_dirprefix + data_location.split('Ana_AM/')[1]\n",
    "os.system(\"mkdir -p \"+local_dir)\n",
    "\n",
    "#define time of save\n",
    "now = datetime.now()\n",
    "save_date = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# permits saving into the new directory we just created\n",
    "#localNewDirName=desiredSavePathName.split('/')[-2]\n",
    "with h5py.File(local_dir+save_date+'even_bound_bs.h5', 'w') as h5file: \n",
    "    h5file.create_dataset(local_dir+save_date+'even_bound_bs', data=even_bound_bs)\n",
    "\n",
    "with h5py.File(local_dir+save_date+'even_unbound_bs.h5', 'w') as h5file: \n",
    "    h5file.create_dataset(local_dir+save_date+'even_unbound_bs', data=even_unbound_bs)\n",
    "\n",
    "with h5py.File(local_dir+save_date+'ratios_conc_ims.h5', 'w') as h5file: \n",
    "    h5file.create_dataset(local_dir+save_date+'ratios_conc_ims', data=ratios_conc_ims)\n",
    "\n",
    "#save the frame interval (time between consecutive images)\n",
    "with h5py.File(local_dir+save_date+'frame_interval.h5', 'w') as h5file: \n",
    "    h5file.create_dataset(local_dir+save_date+'frame_interval', data=frame_interval)\n",
    "    \n",
    "#save the skip interval (the number of frames between analyzed points)\n",
    "with h5py.File(local_dir+save_date+'skip_int.h5', 'w') as h5file: \n",
    "    h5file.create_dataset(local_dir+save_date+'skip_int', data=skip_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mkdir should not overwrite if already exists\n",
    "# desiredSavePathName=data_location.split('data')[0]+'analyzed_data'+data_location.split('data')[1]\n",
    "# os.system(\"mkdir -p \"+desiredSavePathName)\n",
    "\n",
    "# #define time of save\n",
    "# now = datetime.now()\n",
    "# save_date = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# # permits saving into the new directory we just created\n",
    "# localNewDirName=desiredSavePathName.split('/')[-2]\n",
    "# with h5py.File(desiredSavePathName+localNewDirName+save_date+'even_bound_bs.h5', 'w') as h5file: \n",
    "#     h5file.create_dataset(desiredSavePathName+localNewDirName+save_date+'even_bound_bs', data=even_bound_bs)\n",
    "\n",
    "# with h5py.File(desiredSavePathName+localNewDirName+save_date+'even_unbound_bs.h5', 'w') as h5file: \n",
    "#     h5file.create_dataset(desiredSavePathName+localNewDirName+save_date+'even_unbound_bs', data=even_unbound_bs)\n",
    "\n",
    "# with h5py.File(desiredSavePathName+localNewDirName+save_date+'ratios_conc_ims.h5', 'w') as h5file: \n",
    "#     h5file.create_dataset(desiredSavePathName+localNewDirName+save_date+'ratios_conc_ims', data=ratios_conc_ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mkdir should not overwrite if already exists\n",
    "# desiredSavePathName=data_location.split('data')[0]+'analyzed_data'+data_location.split('data')[1]\n",
    "# os.system(\"mkdir -p \"+desiredSavePathName)\n",
    "\n",
    "# #define time of save\n",
    "# now = datetime.now()\n",
    "# save_date = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# # permits saving into the new directory we just created\n",
    "# localNewDirName=desiredSavePathName.split('/')[-2]\n",
    "# with h5py.File(desiredSavePathName+localNewDirName+save_date+'even_bound_bs.h5', 'w') as h5file: \n",
    "#     h5file.create_dataset(desiredSavePathName+localNewDirName+save_date+'even_bound_bs', data=even_bound_bs)\n",
    "\n",
    "# with h5py.File(desiredSavePathName+localNewDirName+save_date+'even_unbound_bs.h5', 'w') as h5file: \n",
    "#     h5file.create_dataset(desiredSavePathName+localNewDirName+save_date+'even_unbound_bs', data=even_unbound_bs)\n",
    "\n",
    "# with h5py.File(desiredSavePathName+localNewDirName+save_date+'ratios_conc_ims.h5', 'w') as h5file: \n",
    "#     h5file.create_dataset(desiredSavePathName+localNewDirName+save_date+'ratios_conc_ims', data=ratios_conc_ims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read h5files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = '../../analyzed_data/atp_hydro/Najma_Hydrolysis/ADP/1_ADP_variation_at_1420uMATP/Nikon_10X_bin1_20sFrameInterval_100ms480_150ms405_1420uATP_1uMmicro_1400nM_A81D_1/5000uMADP/'\n",
    "local_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_h5py(path_name, dateDigits=15, excludes=[]):\n",
    "\n",
    "    # Extract initial conditions of experiment\n",
    "    experiment_type = path_name.split(\"/\")[5]; \n",
    "\n",
    "    # If looking at ADP variation\n",
    "    if experiment_type == \"ADP\": \n",
    "        ATP0 = float(path_name.split(\"uMATP\")[0].split(\"_\")[-1]); \n",
    "        ADP0 = float(path_name.split(\"uMADP\")[0].split(\"/\")[-1]); \n",
    "        P0 = 0; \n",
    "    \n",
    "    # Dictionary to store arrays from each file\n",
    "    all_data = {}\n",
    "    all_data[\"ATP0\"] = ATP0; \n",
    "    all_data[\"ADP0\"] = ADP0; \n",
    "    all_data[\"P0\"] = P0; \n",
    "\n",
    "    # Glob for directory parsing to get all .h5 files in the specified path\n",
    "    h5files = glob.glob(path_name + '/*.h5')\n",
    "    for excl in excludes:\n",
    "        h5files=[file for file in h5files if excl not in file]\n",
    "\n",
    "    for file in h5files:\n",
    "        #print(\"file in h5files\", file)\n",
    "        file_data = []  # List to hold datasets for the current file\n",
    "        \n",
    "        with h5py.File(file, 'r') as h5file:\n",
    "            def load_data(name, obj):\n",
    "                if isinstance(obj, h5py.Dataset):\n",
    "                    file_data.append(obj[:])  # Append dataset as a NumPy array\n",
    "            \n",
    "            # Visit all datasets in the file and load them into file_data\n",
    "            h5file.visititems(load_data)\n",
    "        \n",
    "        #slice by 8 since the date has 8 didgits\n",
    "        field_name = file.split('.h5')[0].split('/')[-1][dateDigits:]\n",
    "        #print(file + '\\n')\n",
    "        #print(field_name+ '\\n')\n",
    "        #print(\"field name\", field_name)\n",
    "        # Add this file's data to all_data with the filename as the key\n",
    "        all_data[field_name] = file_data[0]\n",
    "        \n",
    "        \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = \"../../data/atp_hydro/1000uM_ATP_0_ADP_0_P/\"; # Specify data location\n",
    "desiredSavePathName=data_location.split('data')[0]+'analyzed_data'+data_location.split('data')[1]; \n",
    "desiredSavePathName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processed_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiments = []; \n",
    "\n",
    "local_dir1 = '../../analyzed_data/atp_hydro/Najma_Hydrolysis/ADP/1_ADP_variation_at_1420uMATP/Nikon_10X_bin1_20sFrameInterval_100ms480_150ms405_1420uATP_1uMmicro_1400nM_A81D_1/5000uMADP/'\n",
    "local_dir2 = '../../analyzed_data/atp_hydro/Najma_Hydrolysis/ADP/1_ADP_variation_at_1420uMATP/Nikon_10X_bin1_20sFrameInterval_100ms480_150ms405_1420uATP_1uMmicro_1400nM_A81D_1/0uMADP/'\n",
    "local_dir3 = '../../analyzed_data/atp_hydro/Najma_Hydrolysis/ADP/1_ADP_variation_at_500uMATP/Nikon_10X_bin1_20sFrameInterval__100ms480_150ms405_500uATP_1uMmicro_1400nM_A81D_1/0uMADP/'\n",
    "\n",
    "# TO DO: FIX SPLICING ISSUE WITH LOCAL DIR 3\n",
    "for location in [local_dir1, local_dir2, local_dir3]: \n",
    "    print(location)\n",
    "    post_processed_data = read_h5py(location); \n",
    "    all_experiments.append(post_processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiments[0][\"ADP0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processed_data = read_h5py(local_dir); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processed_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_interval = 20; # in seconds\n",
    "time = np.arange(0, 361*time_interval, time_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(0); \n",
    "plt.title(\"ATP\")\n",
    "plt.figure(1); \n",
    "plt.title(\"log ATP\")\n",
    "\n",
    "\n",
    "for experiment in all_experiments:\n",
    "    mean_ATP = np.mean(experiment['ratios_conc_ims'], axis=(1,2)); \n",
    "    plt.figure(0); \n",
    "    plt.plot(mean_ATP); \n",
    "    \n",
    "    plt.figure(1); \n",
    "    plt.plot(np.log(mean_ATP)); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Hydrolysis Rates\n",
    "\n",
    "Here we follow the two-step minimization process described in the overleaf document. Briefly,\n",
    "\n",
    "Step I: Treat each curve as an exponential decay curve, $ATP(t) = ATP(0) e^{-\\Gamma \\, t}$. For each curve, extract $ATP(0)$ and $\\Gamma$.\n",
    "\n",
    "Step II: Using the set of intial ATP concentrations and $\\Gamma$ from the previous step, extract $\\frac{\\gamma}{K_T}, K_D$ using the expression $\\Gamma = \\frac{\\gamma}{K_T}\\frac{m}{1 + \\frac{ATP(0) + ADP(0)}{K_D}}$. See overleaf for derivation of expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_array = []; \n",
    "log_atp0_array = []; \n",
    "atp0_array = []; \n",
    "adp0_array = []; \n",
    "p0_array = []; \n",
    "\n",
    "plt.figure(0); \n",
    "plt.title(\"Log ATP versus Time\")\n",
    "\n",
    "plt.figure(1); \n",
    "plt.title(\"ATP versus Time\")\n",
    "\n",
    "for experiment in all_experiments: \n",
    "    # Extract mean ATP from post-processed data\n",
    "    mean_ATP = np.mean(experiment['ratios_conc_ims'], axis=(1,2)); \n",
    "    atp0_array.append(experiment[\"ATP0\"]); \n",
    "    adp0_array.append(experiment[\"ADP0\"]);\n",
    "    p0_array.append(experiment[\"P0\"]); \n",
    "\n",
    "    # Take logarithm of mean ATP\n",
    "    log_mean_ATP = np.log(mean_ATP); \n",
    "\n",
    "    # Extract ATP(0), Gamma using linear regression\n",
    "    time_interval = 20; # in seconds\n",
    "    time = np.arange(0, 400*time_interval, time_interval); # Hardcode time assuming 20 second interval, to be replaced. \n",
    "    result = sp.stats.linregress(time, log_mean_ATP)\n",
    "\n",
    "    log_atp0 = result[1]; \n",
    "    gamma = result[0]; \n",
    "\n",
    "    # Save in array\n",
    "    gamma_array.append(gamma); \n",
    "    log_atp0_array.append(log_atp0); \n",
    "\n",
    "\n",
    "    # Plot theoretical curve with experimental curve\n",
    "    plt.figure(0)\n",
    "    plt.plot(time, log_mean_ATP, '-k')\n",
    "    plt.plot(time, log_atp0 + gamma*time)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot(time, mean_ATP, '-k')\n",
    "    plt.plot(time, np.exp(log_atp0 + gamma*time))\n",
    "\n",
    "\n",
    "atp0_array = np.array(atp0_array); \n",
    "adp0_array = np.array(adp0_array); \n",
    "p0_array = np.array(p0_array); \n",
    "\n",
    "gamma_array = np.array(gamma_array)\n",
    "\n",
    "\n",
    "    # TODO: take more frame numbers so that ATP value reaches 0, but skip frames in between to decrease computational expenses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); \n",
    "atp0_plus_adp0_array = np.array(atp0_array + adp0_array)\n",
    "plt.scatter(atp0_array + adp0_array, gamma_array)\n",
    "plt.xlabel(\"ATP0 + ADP0\")\n",
    "plt.ylabel(\"Gamma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model function\n",
    "def slope_functional_form(atp0_plus_adp0, params):\n",
    "    gamma_by_KT, KD = params; \n",
    "    m = 1; # motor concentration\n",
    "    return gamma_by_KT*(1/(1 + (atp0_plus_adp0/KD)))\n",
    "\n",
    "# Define the cost function (squared error) TODO: should we switch to mean square error?\n",
    "def cost_function(params, atp0_plus_adp0, gamma_fitted):\n",
    "    gamma_predicted = slope_functional_form(atp0_plus_adp0, params)\n",
    "    return np.sum((gamma_predicted - gamma_fitted)**2)\n",
    "\n",
    "# Initial guess for the parameters\n",
    "initial_guess = [0.01, 30]; # gamma_by_KT, KD\n",
    "\n",
    "# Minimize the cost function to find the optimal parameters\n",
    "result = minimize(cost_function, initial_guess, args=(atp0_plus_adp0_array, -gamma_array))\n",
    "\n",
    "# Extract the optimal parameters\n",
    "params_optimal = result.x\n",
    "print(f\"Optimal parameters: gamma_by_KT={params_optimal[0]} sec-1 uM-1, KD={params_optimal[1]} uM\")\n",
    "\n",
    "# Plot the original data\n",
    "p = figure(width = 300, height = 300); \n",
    "p.circle(atp0_plus_adp0_array, -gamma_array, legend_label='Experimental Data', alpha = 0.5)\n",
    "\n",
    "# Plot the fitted curve\n",
    "X_fit = atp0_array + adp0_array; \n",
    "X_fit.sort()\n",
    "y_fit = slope_functional_form(X_fit, params_optimal)\n",
    "p.line(X_fit, y_fit, legend_label='Fitted curve', color='darkorange', line_dash=\"dashed\", line_width=1.5)\n",
    "\n",
    "p.xaxis.axis_label = \"Initial Conditions ATP0 + ADP0 (µM)\"; \n",
    "p.yaxis.axis_label = \"Rate of Decay Γ (per sec)\"; \n",
    "\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Post Processing Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data from CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify location of csv file\n",
    "directory = '../../analyzed_data/atp-hydro/';\n",
    "csv_files = ['ATP.csv', 'ADP.csv', 'Phosphate.csv']\n",
    "dataframes = []; \n",
    "\n",
    "for csv in csv_files: \n",
    "    csv_location = directory + csv;\n",
    "    df = pd.read_csv(csv_location);\n",
    "    dataframes.append(df); \n",
    "\n",
    "# Combine all dataframes into one dataframe\n",
    "df_data = pd.concat(dataframes)\n",
    "\n",
    "# Convert string-type ATP curves to array-type. \n",
    "df_data['ATP Curve (uM)'] = df_data[\"ATP Curve (uM)\"].map(lambda curve: np.array(ast.literal_eval(curve))); \n",
    "df_data['Time Array (s)'] = df_data[\"Time Array (s)\"].map(lambda curve: np.array(ast.literal_eval(curve))); \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[\"P Concentration (uM)\"]*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amax(df_data[\"ATP Concentration (uM)\"] + df_data[\"ADP Concentration (uM)\"] + df_data[\"P Concentration (uM)\"]*1e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### ------------- Load and Read Data ------------- ####\n",
    "# ATP_conc_list = []\n",
    "# ADP_conc_list = []\n",
    "# P_conc_list = []\n",
    "# ATP_curve_list = []\n",
    "# ratio_curve_list = []\n",
    "# linear_r2_list = []\n",
    "# exponential_r2_list = []\n",
    "# linear_hydrolysis_rate_list = []\n",
    "# exponential_hydrolysis_rate_list = []\n",
    "# times_list = []\n",
    "# data_locations_list = []\n",
    "\n",
    "# # for df in [df1]:\n",
    "# # for df in [df1, df2, df3]:\n",
    "# for df in dataframes[:1]:  # without phosphate data\n",
    "#     # ATP Concentrations\n",
    "#     ATP_conc_list.append(np.array(df[\"ATP Concentration (uM)\"])); \n",
    "\n",
    "#     # ADP Concentrations\n",
    "#     ADP_conc_list.append(np.array(df[\"ADP Concentration (uM)\"])); \n",
    "\n",
    "#     # Phosphate Concentrations\n",
    "#     P_conc_list.append(np.array(df[\"P Concentration (uM)\"])); \n",
    "\n",
    "#     # ATP Curves\n",
    "#     ATP_curve_list.append([ast.literal_eval(df[\"ATP Curve (uM)\"][i]) for i in range(len(df))])\n",
    "\n",
    "#     # Ratio Curves\n",
    "#     ratio_curve_list.append([ast.literal_eval(df[\"Ratio (A.U.)\"][i]) for i in range(len(df))])\n",
    "\n",
    "#     # Goodness of Fit\n",
    "#     linear_r2_list.append(np.array(df[\"r-squared for linear fit\"])); \n",
    "#     exponential_r2_list.append(np.array(df[\"r-squared for exponential fit\"])); \n",
    "\n",
    "#     # Hydrolysis Rate\n",
    "#     linear_hydrolysis_rate_list.append(np.array(df[\"Hydrolysis Rate (uM/s/motor) from Linear Fitting (-abs(Slope)/Motconc)\"])); \n",
    "#     exponential_hydrolysis_rate_list.append(np.array(df[\"Hydrolysis Rate (uM/s/motor) from Exponential Curve\"])); \n",
    "\n",
    "#     # Time\n",
    "#     times_list.append([ast.literal_eval(df[\"Time Array (s)\"][i]) for i in range(len(df))])\n",
    "    \n",
    "#     # Data location\n",
    "#     data_locations_list.append(df[\"Data Location\"])\n",
    "\n",
    "    \n",
    "# times_list = [item for sublist in times_list for item in sublist];\n",
    "# ATP_conc_list = [item for sublist in ATP_conc_list for item in sublist]; \n",
    "# ADP_conc_list = [item for sublist in ADP_conc_list for item in sublist];\n",
    "# P_conc_list = [item for sublist in P_conc_list for item in sublist];\n",
    "# ATP_curve_list = [item for sublist in ATP_curve_list for item in sublist];\n",
    "# ratio_curve_list = [item for sublist in ratio_curve_list for item in sublist];\n",
    "# linear_r2_list = [item for sublist in linear_r2_list for item in sublist];\n",
    "# exponential_r2_list = [item for sublist in exponential_r2_list for item in sublist];\n",
    "# linear_hydrolysis_rate_list = [item for sublist in linear_hydrolysis_rate_list for item in sublist];\n",
    "# exponential_hydrolysis_rate_list = [item for sublist in exponential_hydrolysis_rate_list for item in sublist];\n",
    "# data_locations_list = [item for sublist in data_locations_list for item in sublist]; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare data\n",
    "# # remove_indices = [12, 26, 27, 46]; # manually select curves to not include\n",
    "# remove_indices = []; \n",
    "\n",
    "# atp_array = []; \n",
    "# ytau_array = []; \n",
    "# atp0_array = []; \n",
    "# adp0_array = []; \n",
    "# p0_array = []; \n",
    "# time_array = []; \n",
    "# size_array = []; \n",
    "\n",
    "# j = 0; \n",
    "# for i, row in data.iterrows():\n",
    "#     if i not in remove_indices: \n",
    "#         # if row[\"atp\"][0]>500: \n",
    "#             if len(row[\"time\"]) > 5:\n",
    "#                 atp_array.append(row[\"atp\"])\n",
    "#                 ytau_array.append(row[\"atp\"][0])\n",
    "#                 atp0_array.append(row[\"atp0\"])\n",
    "#                 adp0_array.append(row[\"adp0\"])\n",
    "#                 p0_array.append(row[\"p0\"])\n",
    "#                 time_array.append(row[\"time\"])\n",
    "#                 size_array.append(len(row[\"atp\"])); \n",
    "                \n",
    "#                 j+=1; \n",
    "\n",
    "#                 # if j > 0: \n",
    "#                 #     break\n",
    "# print(len(atp_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality-Control Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_control(df_data, min_num_of_datapoints = 5): \n",
    "\n",
    "    # Curve should have enough points\n",
    "    quality_controlled_df = df_data[df_data[\"ATP Curve (uM)\"].map(lambda curve: len(curve) > min_num_of_datapoints)]; \n",
    "\n",
    "    # Remove curves with numerical instabilities - to be revisited later.\n",
    "    quality_controlled_df = quality_controlled_df[quality_controlled_df[\"ATP Curve (uM)\"].map(lambda curve: np.all(curve < 1e4))]; \n",
    "\n",
    "    return quality_controlled_df; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map a value to its corresponding color\n",
    "def get_color(value, min_value = 0, max_value = 5000, cmap=cm.viridis):\n",
    "    \n",
    "    # Map value to the range [0, 1] (normalize)\n",
    "    normalized_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "    # Get the color\n",
    "    color = cmap(normalized_value)\n",
    "\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data#['Time Array (s)'].loc[69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = 2; # in hours\n",
    "adp0_array = list(set(df_data[\"ADP Concentration (uM)\"])); # Extract all adp0 values used in dataset\n",
    "\n",
    "# Run Quality Control \n",
    "quality_control_df = quality_control(df_data); \n",
    "\n",
    "for index, row in quality_control_df.iterrows():\n",
    "    atp0 = row['ATP Concentration (uM)'];\n",
    "    adp0 = row['ADP Concentration (uM)'];                  # Read initial ADP concentration of given row.\n",
    "\n",
    "    if adp0 == 0: # Looking at specific adp0 values.\n",
    "        time = row[\"Time Array (s)\"]/3600; # Read time array of given row.\n",
    "        atp_curve = row[\"ATP Curve (uM)\"];           # Read ATP array of given row.\n",
    "        color_from_adp0 = get_color(atp0, max_value=1420);             # Obtain color of curve based on initial adp concentration\n",
    "    \n",
    "        if np.where(time < end_time)[0].size != 0: # Ensure array is not empty.\n",
    "            end_index = np.where(time < end_time)[0][-1]; \n",
    "            print(time[0])\n",
    "        else: \n",
    "            end_index = -1; # If time of data collection is less than end_time, take entire dataset. \n",
    "        \n",
    "        plt.plot(time[:end_index], atp_curve[:end_index], color = color_from_adp0); \n",
    "\n",
    "plt.ylim([0, 1000])\n",
    "plt.xlim([0, 2])\n",
    "\n",
    "plt.xlabel(\"Time (hours)\"); \n",
    "plt.ylabel(\"ATP (μM)\"); \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### ALL ATP VS TIME PLOTS, COLOR CODED BY ADP0 VALUES\n",
    "\n",
    "# color_mapper = LinearColorMapper(palette=Viridis256, low=min(adp0_array), high=list(set(adp0_array))[-2])\n",
    "\n",
    "# # Function to map a value to its corresponding color\n",
    "# def get_color(value, total_array):\n",
    "#     second_largest_value = list(set(total_array))[-2]\n",
    "#     # Map value to the range [0, 1] (normalize)\n",
    "#     normalized_value = (value - min(total_array)) / (second_largest_value - min(total_array))\n",
    "#     # Get the color index from the palette\n",
    "#     index = int(normalized_value * (len(Viridis256) - 1))\n",
    "#     # Get the color code\n",
    "#     color = Viridis256[index]\n",
    "#     return color\n",
    "\n",
    "# start_index = 0; \n",
    "\n",
    "# # Create a ColumnDataSource\n",
    "# source = ColumnDataSource(data=dict(x=time_array, y=atp_array, values=adp0_array))\n",
    "\n",
    "# # Create figure\n",
    "# p = figure(width=400, height=400, title=\"ATP vs Time Curves\")\n",
    "\n",
    "# df_data[[\"Time Array (s)\", \"ATP Curve (uM)\", \"ATP Concentration (uM)\", \"ADP Concentration (uM)\", \"P Concentration (uM)\"]]\n",
    "\n",
    "# for time_data, atp_data, atp0, adp0, p0 in zip(time_array, atp_array, atp0_array, adp0_array, p0_array): \n",
    "\n",
    "#     if np.where(time_data < 2*3600)[0].size != 0: \n",
    "#         end_index = np.where(time_data < 2*3600)[0][-1]; \n",
    "#     else: \n",
    "#         end_index = -1; \n",
    "    \n",
    "#     if adp0 <= 1420: \n",
    "#         # atp = atp_data[start_index:end_index]; \n",
    "#         # time = np.array(time_data[start_index:end_index])/3600; \n",
    "\n",
    "#         atp = atp_data; \n",
    "#         time = np.array(time_data)/3600; \n",
    "\n",
    "#         color = get_color(adp0, adp0_array); \n",
    "\n",
    "#         # Add circle glyphs with color mapping\n",
    "#         p.line(time, atp, color=color)\n",
    "\n",
    " \n",
    "# # Create a color bar\n",
    "# color_bar = ColorBar(color_mapper=color_mapper, ticker=BasicTicker(),\n",
    "#                      formatter=PrintfTickFormatter(format=\"%d\"), \n",
    "#                      title = \"Initial Concetration of ADP (μM)\")\n",
    "\n",
    "# # Add color bar to plot\n",
    "# p.add_layout(color_bar, 'right')\n",
    "# p.xaxis.axis_label = \"Time (hours)\"; \n",
    "# p.yaxis.axis_label = \"ATP (μM)\"; \n",
    "\n",
    "# # show(gridplot([[p3, p4]]))\n",
    "# show(p)\n",
    "# # show(p4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
